Priority, Memory, Timeslice Keys
================================

Are priorities small integers, or keys?

Are timeslices large integers, or keys?

How do we express memory ranges when configuring the MPU?

Memory
------

The kernel *could* accept any old pointer/size pair when configuring an MPU
region.  It would need to check that against the kernel's own requirements: that
the region not cross any forbidden zones.

Alternatively, we could issue keys to memory regions, and give programs the
ability to derive sub-regions from a region.  If the operations on such keys are
carefully constructed, no validation is required except for the very first one
created.

(The system would have to be prevented from minting arbitrary new ones, which is
the first time we've run into that.)

I'm specifically imagining using the brand info to describe the region, so that
all such keys can point to the same kernel object.  That leaves us with few
bits to work with, and in particular, does not leave enough bits to fully
describe an ARMv7-M MPU region -- just an extent of memory.

An extent of memory not including subregion disable bits, which is unfortunate.

Since the kernel is not generally in the business of managing memory, it seems
like *not* using capabilities to model task memory extents would be fine.  The
system could always emulate this if it's desired for the application.


Priorities, Timeslices
----------------------

I think the same thing probably goes here.  The raw interface to Contexts lets
any integer timeslice be written, and lets any valid priority be set.  If the
system wants to change that, it can.

It is not important to the kernel that certain priorities only be used in
certain cases.  Nor is it important that timeslices be derived from some root
such that the CPU is not overcommitted.  These are properties the system might
try to achieve, not the kernel.


Hosting FreeRTOS?
=================

It might be practical to run FreeRTOS atop this kernel.  Why would we do that?

Well, running FreeRTOS unprivileged would make it easier to find certain classes
of bugs.  Keeping portions of the task state in kernel memory would make them
difficult to corrupt.  Using messages for interrupts would limit the privilege
of even the interrupt handlers.

All of FreeRTOS's object handles would become synonyms for keys.  IPC wrappers
would accept them as function arguments and move them into the proper place in
the keyring.

Of course, if the application atop FreeRTOS is written to expect arbitrarily
shared memory and common code, our ability to isolate tasks from one another is
hampered.  The system task might need to leave all of RAM and ROM mapped in all
tasks.

Similarly, if objects and tasks are not clearly associated, the simplest port
might leave the true keys to objects in a master keyring and address them there.
Tasks themselves might not hold any keys at all -- they might be kept by the
system, which exposes operations in terms of indices or handles.

This would allow tasks to forge one anothers' authority, of course, but the same
is true of any FreeRTOS system.

For the static model, FreeRTOS's dynamic object creation operations might pose a
problem.  Though, then again, they might not: as long as enough Contexts are
available in the kernel, the rest is system.

Mutexes?
--------

Okay, here's an interesting one.  FreeRTOS mutexes have priority inheritance.
Can we implement priority inheritance atop the kernel?

- We need to store two priorities for each task: effective and base.
- We need a structure containing all tasks waiting for the mutex, sorted by
  priority, such that we can efficiently determine the highest priority in that
  set.  A heap would work, but a traditional priority-indexed array of waiter
  lists would also work.


Approach #1: emulation
----------------------

A system task maintains the base priority for each task; the effective priority
is plugged into its context.

It also maintains a structure describing each mutex, including a flag indicating
whether it's locked, a context key to the holder, and the prioritized list of
waiters.

(Getting the context key will be interesting.  We'd perhaps need to brand the
syscall key handed to each task.  Alternatively, we could introspect on the
reply key, which has the interesting added property that it lets us prove that
a task is blocked for the muex.

The task processes mutex-related messages in a timely fashion, without masking.
If it receives a mutex lock request and the mutex is locked, it moves the reply
key into a keyring, notes its location in a waiter record, and associates it
with the mutex structure.  (Note that in the current IPC model, once we have
accepted a call message, the caller is stuck and cannot abandon or time out
without being destroyed.)

If the addition of the caller to the structure causes the highest waiter
priority to increase (note: this is trivially detectable in the prioritized
array of lists structure) then we alter the effective priority of the task
holding the mutex.

If a waiter times out, we remove it from the list and recompute the effective
priority.  (Note: timeouts would be a system-level decision involving scheduling
receipt of another message at a later date to wake the task.)

When we receive an unlock message, we (1) verify that it's from the holder, and
(2) recompute the priority before (3) unblocking another caller.

This becomes more complicated, as always, by transitive relationships.

Task A locks mutex 1.

Task B, with higher priority, tries to lock mutex 1.  Task A now inherits B's
priority.

Task C locks mutex 2, and then A tries to lock it.  C now inherits B's priority.

Now B times out.  A's priority is reduced.  Because A is waiting on a mutex,
we must recompute that mutex's priority.  In the extreme, for N tasks, we need
up to N priority recomputations per unlock or timeout event.

Still seems doable.


Approach #2: kernel assisted emulation
--------------------------------------

Prioritized wait queues?  We have those already.  The simplest one is a message
gate.

The system allocates one message gate per mutex.  Folks attempting to lock the
mutex become waiters on this gate, either by sending to it directly, or by
action of the system task.  (Suggests a forwarding operation, which may be
distinct from a send in how it behaves when the send must block.)

When the mutex is available, this gate is opened, and its waiters are candidates
for the next received message.  When we receive a message through this gate, its
caller becomes the mutex holder.

We must determine the highest priority of any remaining waiter on the gate (a
new operation) and assign it to the holder.

So imagine that the gate can send a message to the system (even when it's
closed) to inform it when the highest priority of the waiters changes.  This
might occur due to a timeout, a new message arriving, or a transitive priority
change.  We could process this message by transferring the priority to the
associated mutex holder, which might trigger (at most one) more message.

While this mechanism is a little odd, the result is slick: it provides the
system with a way to incrementally process the transitive priority updates.
If the priority of the messages from the gate is derived somehow from the
priority of the contexts involved, it even makes the process preemptible.

On the other hand, preemptible means non-atomic, which is potentially confusing.
You could easily starve out a mutex priority transfer midway.  Maybe this is
fine -- as long as the thing doing the starving is higher priority than anyone
involved with the mutex, it might actually be desirable.

So what kernel support do we need?

- Message gates, either in the current form or a new variant, must be able to
  inform a task of changes in their situation.
  - "New waiter has appeared"
  - "Existing waiter unblocked"
- We need to be able to query the highest priority waiter on such a gate
  - (Or deliver it with the change messages.)
- We may need to be able to selectively unblock such a waiter.
  - This may just be directed receive.
  - And open receive plus masking might be better.
- We need a forwarding operation that can transfer the caller (app task) to a
  new wait queue without blocking the forwarder (system task).
  - And the caller needs to be interruptible.

That last one is a subtle thing.

Yes, we can forward any call already.  But what states are the tasks in?  Does
the message receiver have a "callers" queue to which the caller is added?  Such
a queue is not necessary in the current design... instead, the caller could stop
being a member of *any* queue and be placed in a "waiting for reply" state, to
be awoken (as always) only by the reply key or some sort of interruption.


Calls, States, and Interruption
===============================

To continue the thoughts from above, here is an area that merits consideration:

I've been thinking about *systems* that treat a caller as committed once the
message has been received, but allow interruption during send-wait.  Is this a
property of the *system* or the *kernel*?  i.e. shall the kernel provide a
method to interrupt a task, reporting a call failure, even after its call
message has been received?  Note that such an interruption would require atomic
cancellation of the reply key.

I think the robust construction of such a system would be difficult; it allows
for forking in a state where I don't think forking makes sense.  But does that
mean I should take steps to *prevent* it?  Probably not.

Here's the motivating example: kernel-assisted mutex emulation.

The system initially receives a lock request and transfers it to a gate.

The timeout specified by the caller should apply at that gate (indeed, the
presumption is that the time blocked waiting for the initial receive by the
system task is negligible, so the timeout is meaningless there).

This means that the system must schedule a message for the future to unblock the
task, if it has not succeeded.  Should this message fire, we must somehow remove
the task from the gate's sender queue without regard to priority.

This sounds like interruption, but it's interruption *after* the message was
originally received.  We could construct a forwarding operation that effectively
"unreceives" the message, but this feels like extra mechanism that's basically
just supporting a policy decision.

As an alternative, just forward the damn message and keep track of whatever
information (likely the context key) is necessary to later interrupt it with an
error.

Note that this mechanism is probably required for Minix-style task monitoring
anyway.  We want to be able to destroy and replace a task whether or not it is
currently waiting for its reply key to be used.

So, then, what does this imply about the state of a task?

- **Sending:** when sending, a task is always sending *to* something.  If that
  something is busy, it must enter a queue of senders, to be processed in
  priority order.  The task's state could simply be membership in this queue.
  This state is unambiguously interruptible.

- **Waiting for Reply:** in this state, the task could be a member of a
  receivers queue on an object standing for its reply key.  This queue would
  (barring bugs) only ever contain one object.  It might be more efficiently
  represented as a state enumeration, so long as the object backing the reply
  key has enough information to locate the task without a queue.  This state
  must be interruptible in a way that revokes the reply key.

We haven't learned anything here about receive states.


Gates
=====

(Early notes referred to this as a "port" by analogy with Mach.  I am
increasingly convinced that Mach is a dangerous source for analogies.  I forget
where I got the term "gate" but I'm pretty sure it's one of Shap's projects.)

A gate is a kernel object that emulates some number of other kernel objects by
cooperating with code running in a context.

When application or system programmers send a message "to" a task, they are
actually sending a message to a gate.  The task will receive the message if:

1. It executes a *directed receive* on the gate.  This will yield the single
   highest priority message on that gate.

2. It executes an *open receive* and the gate is *bound* to the task.

Bound gates and normal gates may be intrinsically different types of objects;
TBD.  A bound gate holds a reference to a task, and the task a list of its bound
gates.  On open receive, the task looks through this list and takes the highest
priority available message from it.  If no such message is available, the task
enters an open receive state and waits.

When a message arrives at a bound gate, it checks if its task is waiting, and,
if so, performs a thread migration into the task.  Note that at most one task
can be waiting.

When a message is delivered from a bound gate, the task receives information
about which gate it arrived through (the message direction).  What information
isn't immediately clear; a gate key is wrong, because normal tasks can't
meaningfully compare keys.  An object table index would uniquely identify the
gate, but would not be meaningful to application tasks that don't manipulate the
object table.  So perhaps the gate should have a configured integer it passes
into the task.  This is in addition to the brand: brands are still valuable
in the presence of direction info (see my previous notes).  But this does make
the result ABI of a bound gate receive possibly different from a directed
receive....

Non-bound (normal) gates are similar except that they cannot participate in
open receive, and they maintain a list of waiting tasks.  When a message
arrives, the kernel migrates into the highest priority task among the waiters.

It may or may not be possible to do directed receive on a bound gate.  If the
gate is bound to some other task, it's definitely an error.  I'd need a
motivating use case to support directed receive for bound gates.

It is physically impossible to execute an open receive "on" a non-bound gate:
the open receive does not take a gate argument.

Gate Groups?
------------

On consideration, perhaps we should eliminate the concept of open receive (!)
and switch to an abstraction I'm calling (for now) a "gate group."  A gate group
is a (wait for it) group of gates.

A task can execute a directed receive on the gate group.  At the time of the
receive, the gate group would inspect the wait list of each associated gate,
and deliver the highest priority waiting message.  If no such message exists,
the receiving task blocks in directed recieve on the group itself.

When a message arrives at a gate that is a member of a group, it would search
the group's list of waiters and attempt to migrate directly.  If that fails, the
sending task blocks on the gate.

This feels like a cleaner solution, primarily because there's now a wait list
for the receiving task to block in (the gate's).  This eliminates the need for a
distinct open receive state -- and, indeed, the need for an open receive
operation at all!

We could maintain the direction words by making them part of the standard
receive ABI.  Perhaps *every* gate has an associated "gate brand" that would be
delivered to the task alongside the "message brand."

Ooooooh... okay... this also makes the concept of opening and closing gates
cleaner.  Open and closing a gate in a group would be accomplished by a message
to the *group*.  The group serves as a control object for the task that uses it.

Also nice: multiple tasks could service a gate group.

Yes, this is better.  Let's do this.


Reply Gates
-----------

Every key refers to an object in the object table.  Entries in the table are the
units of revocation and versioning -- we can atomically revoke all the keys to a
table entry.

We need to be able to revoke a reply key without revoking keys to the
corresponding context.  Thus the reply key cannot be a context key with e.g. a
special brand.

This all implies that a context's reply key must refer to a distinct but
associated kernel object.  Because messages to tasks go through gates, it seems
clear that this object is a *reply gate.*

Each context could have an associated reply gate.  On a call from the object, a
key to the reply gate would be minted.  On use of that key, the reply gate would
deliver the message to the task and version itself, atomically invalidating all
extant keys.  The task issuing the call would block in directed receive on its
reply gate.  Anyone else attempting to receive from the reply gate key should
fail.

If interruption causes the task to unblock, we would also version its reply
gate.  If the task was blocked in a call, its reply key is now invalid; if it
was blocked for some other reason, this operation is still pretty cheap.

Revocation of reply keys remains cheap, because no task ever blocks at the reply
gate.  Observe:
- We revoke reply keys whenever the associated task is not blocked in a call.
- Only if the task is doing something other than blocking in a call could others
  block on the reply key.
- Delivery of a reply message results in immediate migration and does not block.


Message Senders
===============

The current implementation assumes that a message comes from a task.  This isn't
really true.  Interrupts, for one, can send messages.  The asynchronous message
holder thing I've talked about would be able to send messages.

Any object that sends messages must support the following:
- Some way of getting the contents of message -- data and keys.
- A list node, so that it can block if the recipient is not ready.
- A priority, so that it can be prioritized among other outstanding messages.


Message Receivers
=================

In the current design, only contexts can block to receive messages.  There is no
concept of an asynchronous message receiver (that's basically what a gate is).

So we don't need an abstraction atop context for this, I think.


Do we need other types of lists?
================================

We need lists to describe:
- Who is waiting at a gate to deliver a message.
- Who is waiting at a gate to receive a message.
- Which contexts are currently runnable.

Many types of kernel objects never need to be a member of these lists, so it
doesn't make sense to have every object have a list node.  We appear to only
need lists of message senders and contexts.

For type-safety, Contexts could contain two list nodes: one that is used when
the context joins a list of Senders, and one that is used when the context joins
a list of contexts.  Technically this is wasteful, but it does eliminate some
casting.

Hmmm.  The kernel clock driver may need to maintain a list of timers, sorted
by deadline.
